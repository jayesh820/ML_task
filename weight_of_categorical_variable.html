<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width,initial-scale=1" />
  <title>What Happens to the Dropped Category? — Advanced Guide to Dummy Encoding</title>
  <link href="https://cdn.jsdelivr.net/npm/tailwindcss@2.2.19/dist/tailwind.min.css" rel="stylesheet">
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;600;700&display=swap" rel="stylesheet">
  <style>
    body{font-family:Inter,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial,sans-serif;background:#f8fafc;color:#0f172a}
    .card{box-shadow:0 8px 30px rgba(2,6,23,.06);background:white;border-radius:12px;padding:1.25rem}
    pre{background:#0b1220;color:#e6edf3;padding:1rem;border-radius:8px;overflow:auto}
    table{border-collapse:collapse;width:100%}
    td,th{border:1px solid #e6e9ee;padding:.6rem;text-align:left}
    .muted{color:#64748b}
    .kpi{background:#eef2ff;color:#312e81;padding:.25rem .5rem;border-radius:999px;font-weight:600}
    .svg-diagram{width:100%;height:auto}
  </style>
</head>
<body class="p-6">
  <div class="max-w-5xl mx-auto">
    <header class="mb-6">
      <div class="card">
        <h1 class="text-2xl font-bold">What Happens to the Dropped Category?</h1>
        <p class="muted mt-2">A focused technical walkthrough of dummy / one-hot encoding with dropped categories, how weights are interpreted, practical examples (linear & logistic regression), encoding code snippets, and visualization of effects.</p>
      </div>
    </header>

    <section class="mb-6">
      <div class="card">
        <h2 class="text-xl font-semibold">1. Context — Dummy / One‑Hot Encoding with a Dropped Category</h2>
        <p class="mt-2 muted">In linear and logistic regression we convert a categorical variable with <em>k</em> levels to <em>k−1</em> indicator variables by dropping one level. This prevents perfect multicollinearity (the dummy variable trap) and makes the dropped level the baseline (reference) category.</p>

        <h3 class="mt-3 font-medium">Why drop one?</h3>
        <p class="muted">If you keep all <em>k</em> indicators, the design matrix columns will sum to the constant vector 1 for every sample (assuming the categories partition the data). That makes the matrix singular and the ordinary least squares solution undefined.</p>
      </div>
    </section>

    <section class="mb-6">
      <div class="card">
        <h2 class="text-xl font-semibold">2. What happens to the weights — Intuition & Math</h2>
        <p class="mt-2 muted">When you drop category <strong>C<sub>0</sub></strong>, you don't literally set its coefficient to 0; rather the model absorbs the baseline into the intercept. The coefficients for the remaining one-hot columns measure differences relative to that baseline.</p>

        <h3 class="mt-3 font-medium">Linear model example</h3>
        <p class="muted">Model: <code>y = β₀ + β₁⋅I(Blue) + β₂⋅I(Green) + ε</code><br>
           If the dropped category is <strong>Red</strong>, then:</p>
        <ul class="mt-2 muted">
          <li>Prediction for Red: <code>ŷ = β₀</code></li>
          <li>Prediction for Blue: <code>ŷ = β₀ + β₁</code></li>
          <li>Prediction for Green: <code>ŷ = β₀ + β₂</code></li>
        </ul>

        <h3 class="mt-3 font-medium">Logistic regression (classification)</h3>
        <p class="muted">If <code>logit(p) = β₀ + β₁⋅I(Blue) + β₂⋅I(Green)</code>, the odds for the baseline (Red) are <code>exp(β₀)</code>, and <code>exp(β₁)</code> is the odds ratio of Blue vs Red.</p>

        <div class="mt-4">
          <svg class="svg-diagram" viewBox="0 0 800 130" xmlns="http://www.w3.org/2000/svg">
            <style>.t{font:14px/1.2 Inter, sans-serif;fill:#0f172a}.mut{fill:#64748b;font:12px/1.2 Inter}</style>
            <rect x="10" y="10" width="240" height="110" rx="8" fill="#eef2ff"/>
            <text x="30" y="36" class="t">Input category: Color</text>
            <text x="30" y="58" class="mut">Levels: Red, Blue, Green</text>

            <g transform="translate(270,10)">
              <rect x="0" y="0" width="230" height="110" rx="8" fill="#fff7ed"/>
              <text x="12" y="26" class="t">One‑hot (drop Red)</text>
              <text x="12" y="48" class="mut">Blue → [1,0]</text>
              <text x="12" y="68" class="mut">Green → [0,1]</text>
              <text x="12" y="88" class="mut">Red → [0,0] (baseline)</text>
            </g>

            <g transform="translate(520,10)">
              <rect x="0" y="0" width="260" height="110" rx="8" fill="#e6fffa"/>
              <text x="12" y="26" class="t">Linear model</text>
              <text x="12" y="52" class="mut">ŷ = β₀ + β₁⋅I(Blue) + β₂⋅I(Green)</text>
              <text x="12" y="76" class="mut">Interpretation: β₁ = effect of Blue vs Red</text>
            </g>

            <line x1="250" y1="65" x2="270" y2="65" stroke="#c7d2fe" stroke-width="2"/>
            <line x1="500" y1="65" x2="520" y2="65" stroke="#bbf7d0" stroke-width="2"/>
          </svg>
        </div>
      </div>
    </section>

    <section class="mb-6">
      <div class="card">
        <h2 class="text-xl font-semibold">3. If you keep all categories (don’t drop one)</h2>
        <p class="muted mt-2">Keeping all <em>k</em> indicators causes perfect multicollinearity — the design matrix becomes singular and OLS has no unique solution. Most libraries (scikit-learn, statsmodels) either drop one column automatically or raise an error.</p>
      </div>
    </section>

    <section class="mb-6">
      <div class="card">
        <h2 class="text-xl font-semibold">4. Tree‑based models and embedding approaches</h2>
        <p class="muted mt-2">Tree models (Decision Trees, Random Forests, Gradient Boosting) are robust to full one-hot encodings — they can handle all categories because they split on membership. For high-cardinality categorical variables, embedding layers (neural nets) and target encoding are common alternatives.</p>

        <h4 class="mt-3 font-medium">Best practice summary</h4>
        <ul class="mt-2 muted">
          <li>Linear/logistic models: drop one category (explicit baseline).</li>
          <li>Regularized linear models: still drop one to avoid numerical instability.</li>
          <li>Tree models: you may keep all dummies or use categorical splits directly.</li>
          <li>Neural networks: prefer embeddings for high-cardinality features.</li>
        </ul>
      </div>
    </section>

    <section class="mb-6">
      <div class="card">
        <h2 class="text-xl font-semibold">5. Code — Practical examples (pandas / scikit‑learn / statsmodels)</h2>
        <p class="muted mt-2">Quick snippets showing common workflows.</p>

        <h4 class="mt-3 font-medium">pandas (drop first)</h4>
        <pre><code>import pandas as pd

df = pd.DataFrame({'Color':['Red','Blue','Green','Blue']})
df_ohe = pd.get_dummies(df['Color'], prefix='Color', drop_first=True)
print(df_ohe)
# Blue Green
#   0    0 0
#   1    1 0
#   2    0 1
#   3    1 0
</code></pre>

        <h4 class="mt-3 font-medium">scikit-learn OneHotEncoder (drop='first')</h4>
        <pre><code>from sklearn.preprocessing import OneHotEncoder
enc = OneHotEncoder(drop='first', sparse=False)
X = enc.fit_transform(df[['Color']])
</code></pre>

        <h4 class="mt-3 font-medium">statsmodels OLS — automatically handled if you use formulas</h4>
        <pre><code>import statsmodels.formula.api as smf
import pandas as pd

df = pd.DataFrame({'y':[10,12,9,13],'Color':['Red','Blue','Green','Blue']})
model = smf.ols('y ~ C(Color)', data=df).fit()
print(model.summary())
# statsmodels drops one level (baseline) and reports coefficients relative to it.
</code></pre>
      </div>
    </section>

    <section class="mb-6">
      <div class="card">
        <h2 class="text-xl font-semibold">6. Interpretability & reporting</h2>
        <p class="muted mt-2">When you report coefficients, always state the baseline. Example table:</p>
        <table class="mt-3">
          <thead><tr><th>Feature</th><th>Coef (β)</th><th>Interpretation</th></tr></thead>
          <tbody>
            <tr><td>Intercept</td><td>10</td><td>Base prediction for Red</td></tr>
            <tr><td>Color_Blue</td><td>+2</td><td>Blue is +2 vs Red</td></tr>
            <tr><td>Color_Green</td><td>-1</td><td>Green is -1 vs Red</td></tr>
          </tbody>
        </table>
        <p class="muted mt-3">If you change the dropped category (e.g., drop Green instead), the coefficients and intercept reparameterize accordingly — they measure new differences relative to the new baseline.</p>
      </div>
    </section>

    <section class="mb-6">
      <div class="card">
        <h2 class="text-xl font-semibold">7. Advanced notes</h2>
        <ul class="mt-2 muted">
          <li><strong>Reference coding alternatives:</strong> Effect coding (sum-to-zero) sets coefficients so they sum to zero and interprets parameters differently; useful in ANOVA contexts.</li>
          <li><strong>Target encoding:</strong> Replace categories by aggregated target stats — can leak and needs smoothing/cv.</li>
          <li><strong>Interaction terms:</strong> When interacting categorical variables, be careful how you drop levels; interactions change the baseline interpretation.</li>
          <li><strong>Centering predictors:</strong> Good practice to center continuous covariates to make intercepts meaningful.</li>
        </ul>
      </div>
    </section>

    <section class="mb-6">
      <div class="card">
        <h2 class="text-xl font-semibold">8. Quick checklist for practitioners</h2>
        <ol class="mt-2 muted">
          <li>Decide model family (linear vs tree vs NN).</li>
          <li>For linear models, pick and document baseline (drop one).</li>
          <li>For high-cardinality categorical features, prefer embeddings or target encoding with regularization.</li>
          <li>Check multicollinearity / condition number if you keep all dummies by mistake.</li>
          <li>When reporting, always include baseline and explain coefficient signs and magnitudes in context.</li>
        </ol>
      </div>
    </section>

    <footer class="mt-6 text-sm muted text-center">© 2025 — Guide: Dropped Category & Dummy Encoding • Built for analysts & ML engineers</footer>
  </div>
</body>
</html>
s